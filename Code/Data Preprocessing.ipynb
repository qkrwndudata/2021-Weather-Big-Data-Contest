{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 내부 데이터 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rc('font', family='NanumBarunGothic') \n",
    "from matplotlib.colors import rgb2hex\n",
    "from matplotlib.colors import ListedColormap\n",
    "import matplotlib\n",
    "get_ipython().run_line_magic('config', \"InlineBackend.figure_format='retina' #화질 좋게 해주기\")\n",
    "import plotly.express as px\n",
    "import plotly.offline as pyo\n",
    "import plotly.graph_objs as go\n",
    "import sklearn\n",
    "import seaborn as sb\n",
    "import plotly.io as pio\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore') \n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "\n",
    "from scipy.stats import norm\n",
    "import scipy.stats as stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dat = '/content/gdrive/Shareddrives/날씨 빅데이터 콘테스트/내부 데이터/'\n",
    "\n",
    "# 데이터 읽는 함수 만들기\n",
    "\n",
    "buycol = ['date', 'sex', 'age', 'bigcat', 'smcat', 'qty']\n",
    "snscol = ['date', 'bigcat', 'smcat', 'cnt']\n",
    "\n",
    "def read_buy(df):\n",
    "    df = pd.read_csv(df +\".csv\")\n",
    "    df = df.drop('Unnamed: 0', axis=1)\n",
    "    df.columns = buycol\n",
    "    return df\n",
    "    \n",
    "def read_sns(df):\n",
    "    df = pd.read_csv(df +\".csv\")\n",
    "    df = df.drop('Unnamed: 0', axis=1)\n",
    "    df.columns = snscol\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "buy1 = read_buy(dat+\"buy2018_1\")\n",
    "buy2 = read_buy(dat+\"buy2018_2\")\n",
    "buy3 = read_buy(dat+\"buy2019_1\")\n",
    "buy4 = read_buy(dat+\"buy2019_2\")\n",
    "buy = pd.concat([buy1, buy2, buy3, buy4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns1 = read_sns(dat+\"sns2018_1\")\n",
    "sns2 = read_sns(dat+\"sns2018_2\")\n",
    "sns3 = read_sns(dat+\"sns2019_1\")\n",
    "sns4 = read_sns(dat+\"sns2019_2\")\n",
    "sns = pd.concat([sns1, sns2, sns3, sns4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# datetime으로 바꾸기\n",
    "from datetime import datetime\n",
    "buy['date'] = pd.to_datetime(buy['date'], format='%Y%m%d')\n",
    "sns['date'] = pd.to_datetime(sns['date'], format='%Y%m%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 각 카테고리 별 총합\n",
    "fig, ax = plt.subplots(1,2,figsize=(20,5))\n",
    "\n",
    "# 주별로 묶기 전\n",
    "df = buy.groupby(['date','bigcat']).sum().reset_index()\n",
    "for i in df['bigcat'].unique().tolist():\n",
    "    d_ = df[(df[\"bigcat\"]==i)]\n",
    "    ax[0].plot(d_[\"date\"], d_[\"qty\"], \"-\", alpha=.6)\n",
    "ax[0].set_title('daily')\n",
    "\n",
    "# 주별로 묶은 뒤\n",
    "df = df.set_index('date')\n",
    "plt.figure(figsize=(10,5))\n",
    "for i in df['bigcat'].unique().tolist():\n",
    "    d_ = df[(df[\"bigcat\"]==i)]\n",
    "    d_ = d_.resample('W', label='left').mean().reset_index() # 주 단위로 묶어주기\n",
    "    ax[1].plot(d_[\"date\"], d_[\"qty\"], \"-\", alpha=.6)\n",
    "ax[1].set_title('weekly')\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 외부 데이터 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 사전에 전처리한 데이터들 합차기\n",
    "df1 = pd.read_csv('/content/gdrive/Shareddrives/날씨 빅데이터 콘테스트/외부 데이터/전처리 후 데이터/날씨 데이터/기온강수.csv')\n",
    "df2 = pd.read_csv('/content/gdrive/Shareddrives/날씨 빅데이터 콘테스트/외부 데이터/전처리 후 데이터/날씨 데이터/기온습도열지수_서울.csv')\n",
    "df3 = pd.read_csv('/content/gdrive/Shareddrives/날씨 빅데이터 콘테스트/외부 데이터/전처리 후 데이터/날씨 데이터/대기오염_서울.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# datetime으로 바꾸기\n",
    "from datetime import datetime\n",
    "df2['일자'] = pd.to_datetime(df2['일자'], format='%Y%m%d')\n",
    "\n",
    "# merge를 위해 str로 바꾸기\n",
    "df1['일자'] = df1['일시'].astype(str)\n",
    "df2['일자'] = df2['일자'].astype(str)\n",
    "df3['일자'] = df3['측정일자'].astype(str)\n",
    "\n",
    "# 하나의 데이터로 합치기\n",
    "from functools import reduce\n",
    "dfs = [df1,df2,df3]\n",
    "df_merge = reduce(lambda left, right: pd.merge(left, right, on='일자', how='outer'), dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 필요없는 컬럼 삭제\n",
    "df_merge = df_merge.drop(['Unnamed: 0_x','Unnamed: 0_y','측정일자', '일자','기온(°C)_y'], axis=1)\n",
    "\n",
    "# 컬럼명 변경\n",
    "df_merge = df_merge.rename({'일시':'일자', '기온(°C)_x':'기온(°C)'}, axis='columns')\n",
    "\n",
    "# datetime으로 바꾸기\n",
    "df_merge['일자'] = pd.to_datetime(df_merge['일자'], format='%Y-%m-%d')\n",
    "df_merge.to_csv('climate.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 날씨자료 수정(대기오염)\n",
    "data_2018 = pd.read_csv('/content/gdrive/Shareddrives/날씨 빅데이터 콘테스트/외부 데이터/원본 데이터/날씨 데이터/대기오염/일평균_대기환경_정보_2018년.csv')\n",
    "data_2019 = pd.read_csv('/content/gdrive/Shareddrives/날씨 빅데이터 콘테스트/외부 데이터/원본 데이터/날씨 데이터/대기오염/일평균_대기환경_정보_2019년.csv')\n",
    "\n",
    "# 자료 합치기\n",
    "data_air = pd.concat([data_2018, data_2019], axis=0)\n",
    "data_air = data_air.loc[:,data_air.columns[2:]]\n",
    "data_air.columns = ['미세먼지(㎍/㎥)', '초미세먼지(㎍/㎥)', '오존(ppm)', '이산화질소농도(ppm)', '일산화탄소농도(ppm)', '아황산가스농도(ppm)']\n",
    "data_air.reset_index(drop=True,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 기존 climate 불러오기\n",
    "df_climate = pd.read_csv('/content/gdrive/Shareddrives/날씨 빅데이터 콘테스트/외부 데이터/전처리 후 데이터/날씨 데이터/climate.csv')\n",
    "\n",
    "# 기존 climate 분할\n",
    "df_climate_1 = df_climate.loc[:,df_climate.columns[1:9]]\n",
    "df_climate_2 = df_climate.loc[:,df_climate.columns[15:17]]\n",
    "\n",
    "# 수정된 자료 넣고 합치기\n",
    "data_climate = [df_climate_1, data_air, df_climate_2]\n",
    "data_climate = reduce(lambda  left,right: pd.concat([left,right],\n",
    "                                            axis=1), data_climate)\n",
    "data_climate\n",
    "\n",
    "#os.chdir('/content/gdrive/Shareddrives/날씨 빅데이터 콘테스트/외부 데이터/전처리 후 데이터/날씨 데이터')\n",
    "#data_climate.to_csv('climate2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 자료 추가 (풍속, 구름 데이터)\n",
    "data_2018 = pd.read_csv(\"/content/gdrive/Shareddrives/날씨 빅데이터 콘테스트/외부 데이터/원본 데이터/날씨 데이터/2018풍속눈구름.csv\", encoding='cp949')\n",
    "data_2019 = pd.read_csv(\"/content/gdrive/Shareddrives/날씨 빅데이터 콘테스트/외부 데이터/원본 데이터/날씨 데이터/2019풍속눈구름.csv\", encoding='cp949')\n",
    "\n",
    "data_all = pd.concat([data_2018, data_2019], ignore_index=True)\n",
    "data_all = data_all.drop(['적설(cm)', '지점', '지점명'], axis=1)\n",
    "\n",
    "from datetime import datetime\n",
    "data_all['일시'] = pd.to_datetime(data_all['일시'])\n",
    "data_all['일시'] = data_all['일시'].dt.floor('D')\n",
    "\n",
    "# 결측치는 0으로 간주\n",
    "data_all = data_all.fillna(0)\n",
    "\n",
    "# 시간 별 풍속, 전운량을 평균내어 일별 데이터로 생성\n",
    "newdata = data_all.groupby('일시').mean().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 최종데이터 climate3\n",
    "climate3 = pd.concat([data_climate, newdata], axis = 1)\n",
    "climate3 = climate3.drop(['일시'], axis=1)\n",
    "climate3\n",
    "# climate3.to_csv('/content/gdrive/Shareddrives/날씨 빅데이터 콘테스트/외부 데이터/전처리 후 데이터/날씨 데이터/climate3.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 모델링을 위한 데이터 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drink만 따로 추출\n",
    "buy_drink = buy[buy['smcat'].str.contains('차|아이스티|밀크티|음료|커피|코코아|즙|식혜|요거트|탄산수|우유|주스|밀크|음용식초|두유|생수')].reset_index()\n",
    "buy_drink = buy_drink.drop(['index','bigcat'], axis=1)\n",
    "\n",
    "sns_drink = sns[sns['smcat'].str.contains('차|아이스티|밀크티|음료|커피|코코아|즙|식혜|요거트|탄산수|우유|주스|밀크|음용식초|두유|생수')].reset_index()\n",
    "sns_drink = sns_drink.drop(['index','bigcat'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 날씨 데이터 불러오기\n",
    "climate = pd.read_csv('/content/gdrive/Shareddrives/날씨 빅데이터 콘테스트/외부 데이터/전처리 후 데이터/날씨 데이터/climate3.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge하기 위해 따로 string으로 바꿔주기\n",
    "buy_drink['date'] = buy_drink['date'].astype(str)\n",
    "sns_drink['date'] = sns_drink['date'].astype(str)\n",
    "climate['일자'] = climate['일자'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 구축\n",
    "all = pd.merge(buy_drink, sns_drink, on=['date','smcat'])\n",
    "df = buy_drink.groupby(['date', 'sex', 'age']).count().reset_index()\n",
    "df = df.drop(['smcat', 'qty'], axis=1)\n",
    "\n",
    "for i in buy_drink['smcat'].unique().tolist():\n",
    "  df1 = all[all['smcat']==i]\n",
    "  df = pd.merge(df, df1, on=['date','sex','age'], how='outer')\n",
    "  df = df.drop('smcat', axis=1)\n",
    "  cntname = i + ' sns'\n",
    "  df = df.rename({'qty': i, 'cnt': cntname}, axis='columns')\n",
    "\n",
    "# 결측치 처리된 데이터 0으로 바꿔주기\n",
    "model_drink_data = df.fillna(0)\n",
    "model_drink_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_drink_data.to_csv('/content/gdrive/Shareddrives/날씨 빅데이터 콘테스트/외부 데이터/전처리 후 데이터/모델링용 데이터/model_drink_data.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
